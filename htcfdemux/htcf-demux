#!/usr/bin/env python3

import argparse
import datetime
import os
import threading
from htcfdemux import chunker
from mpi4py import MPI

comm = MPI.COMM_WORLD

TAG_INIT = 0
TAG_CHUNK = 1
TAG_WRITE = 2
TAG_RESPONSE = 3
TAG_DONE = 4

class Handler(threading.Thread):
    def __init__(self, seqfiles, bcfile, outdir):
        threading.Thread.__init__(self)
        self.bcfile = bcfile
        self.outdir = outdir
        self.seqfiles = seqfiles
        self.outfiles = {}

        if not os.path.isdir(self.outdir):
            os.mkdir(self.outdir)

        for name,bc in [l.strip().split() for l in open(self.bcfile)]:
            for i,sf in enumerate(self.seqfiles):
                if len(self.seqfiles) == 1:
                    fname = os.path.join(self.outdir, "%s.fastq" % (name))
                else:
                    fname = os.path.join(self.outdir, "%s_R%d.fastq" % (name, i+1))

                open(fname, 'w') # create and ensure empty files
                self.outfiles[(bc,i)] = {'name': name, 'pos': 0, 'filename': fname }

        self.chunklist = list(chunker.FastqChunker(self.seqfiles[0], chunksize=1024*1024*50).chunks())

    def run(self):
        while 1:
            send_data = None
            s = MPI.Status()
            data = comm.recv(status=s)
            if s.tag == TAG_DONE:
                break
            elif s.tag == TAG_INIT:
                send_data = {'seqfiles': self.seqfiles, 'outfiles': self.outfiles}
            elif s.tag == TAG_CHUNK:
                if len(self.chunklist) > 0:
                    send_data = self.chunklist.pop(0)
                else:
                    send_data = None
            elif s.tag == TAG_WRITE:
                key = data['bc_key']
                send_data = self.outfiles[key]['pos']
                self.outfiles[key]['pos'] += data['size']
            else:
                raise Exception("Unknown TAG received.")
    
            comm.send(send_data, s.source, tag=TAG_RESPONSE)

class Worker(object):
    def __init__(self, comm, debug=False):
        self.comm = comm
        self.debug = debug
        if self.debug:
            self.logfile = open('rank%d.log' % comm.rank, 'w')

    def log(self, msg):
        if self.debug:
            self.logfile.write('%s %s\n' % (datetime.datetime.now(), msg))
            self.logfile.flush()

    def prep(self):
        data = self.comm.sendrecv(None, 0, sendtag=TAG_INIT, recvtag=TAG_RESPONSE)
        self.seqfiles = data['seqfiles']
        self.outfiles = data['outfiles']
        have_multiple_seqs = len(self.seqfiles) > 1
        self.files = []

        for sf in self.seqfiles:
            self.files.append(open(sf, 'rb'))

        for key,attrs in self.outfiles.items():
            attrs['file'] = open(attrs['filename'], 'r+b')


    def cleanup(self):
        for key, attrs in self.outfiles.items():
            self.write_seq(key, force=True)
            #if len(self.files) > 1:
                #for i in range(len(self.files)):
            #else:
                #self.write_seq(b, force=True)
            attrs['file'].close()

    def start(self):
        self.prep()
        while 1:
            self.log("getting chunk")
            chunk = self.comm.sendrecv(None, 0, sendtag=TAG_CHUNK, recvtag=TAG_RESPONSE)
            self.log("got chunk %s" % (str(chunk)))
            if chunk is None: break
            self.log("starting chunk %s" % (str(chunk)))
            for i, f in enumerate(self.files):
                self.process_chunk(chunk, i)
            self.log("ending chunk %s" % (str(chunk)))
        self.cleanup()


    def write_seq(self, key, seq=None, force=False):
        attrs = self.outfiles[key]
        if not attrs.get('buffer', None):
            attrs['buffer'] = bytearray()
        buf = attrs['buffer']
        if seq:
            buf.extend(b"%s" % seq)
        length = len(buf)
        if (not force) and length < 1024*1024*10:
            return
        pos =  self.comm.sendrecv({'bc_key': key, 'size': length} , 0, sendtag=TAG_WRITE, recvtag=TAG_RESPONSE)
        attrs['file'].seek(pos)
        attrs['file'].write(buf)
        attrs['buffer'] = None

    def process_chunk(self, chunk, files_index, index=None):
        offset,length = chunk
        fobj = self.files[files_index]
        fobj.seek(offset)
    
        nonmatching_headers_count = 0    
        total_reads = 0
        matching_bcs = 0
        mismatched_bcs = 0
        #left = length
        #while left:
        #    size = self.buffsize
        #    if left - size < 0:
        #        size = left
        #    self.f1.read(size)
        #    self.f2.read(size)
        #    left = left-size
        #return
        pos = 0
    
        while pos < (length - 1):
            header = fobj.readline()
            read = fobj.readline()
            delimiter = fobj.readline()
            qual = fobj.readline()

            pos += sum(map(len, (header, read, delimiter, qual)))

            key = (read[0:8].decode('utf-8'), files_index)

            if key in self.outfiles:
                self.write_seq(key, b"%s%s%s%s" % (header, read, delimiter, qual))

#        for line in self.f1:
#            pos += len(line)
#            if pos > length:
#                break
#            # If it's a header line
#            if line.startswith(b'@'):
#                total_reads += 1
#                
#                read = self.f1.readline()
#                pos += len(read)
#
#                # Get the barcodes from the sequences (assumes 8 bp)
#                read_bc = read[0:8].decode('utf-8')
#                
#                #Skip the '+' line in the fastq
#                delim = self.f1.readline()
#                pos += len(delim)
#                
#                # Read the qual lines
#                read_qual = self.f1.readline()
#                pos += len(read_qual)
#                
#                
#                key = (read_bc)
#                # Check that the barcode was in the barcode file.
#                if key in self.barcodes:
#                #     Increment the counts for that barcode
#                #    bc_forthisread = bcseq_hash[read1_bc]
#                #    bc_readcounts[bc_forthisread] += 1
#                #    
#                #    # Print these to the appropriate files (if those files exist).
#                #    bc_id = bcseq_hash[read1_bc]
#                #    
#                #    bc_r1_outpath = outpath + '/' + bc_id + '_r1.fastq'
#                #    bc_r2_outpath = outpath + '/' + bc_id + '_r2.fastq'
#                #    
#                         
#                     self.write_seq(key, b"%s%s+\n%s" % (line, read, read_qual))
#                #    if os.path.exists(bc_r1_outpath):
#                #        tempout = filebuffers[bc_r1_outpath]
#                #        tempout.write(r1_line)
#                #        tempout.write(read1)
#                #        tempout.write('+\n')
#                #        tempout.write(read1_qual)
#                #    
#                #        
#                #    if os.path.exists(bc_r2_outpath):
#                #        tempout2 = filebuffers[bc_r2_outpath]
#                #        tempout2.write(r2_line)
#                #        tempout2.write(read2)
#                #        tempout2.write('+\n')
#                #        tempout2.write(read2_qual)
#                
#        #percent_matched = (float(matching_bcs)/float(total_reads))*100
#        #percent_mismatched = (float(mismatched_bcs)/float(total_reads))*100
        #percent_nonmatchingheaders = (float(nonmatching_headers_count)/float(total_reads))*100
   # 
   #     # Print out the summary statistics
   #     summarypath = outpath + '/split_barcodes_summary.log'
   #     summary = open(summarypath, 'w')
   #     summary.write('Read 1 file:\t' + r1_path + '\n')
   #     summary.write('Read 2 file:\t' + r2_path + '\n')
   #     summary.write('Total Reads:\t' + str(total_reads) + '\n')
   #     summary.write('Reads with matching barcodes:\t' + str(matching_bcs) + '\n')
   #     summary.write('Percent with matching barcodes:\t' + str(percent_matched) + '\n')
   #     summary.write('Reads with mismatched barcodes:\t' + str(mismatched_bcs) + '\n')
   #     summary.write('Percent with mismatched barcodes:\t' + str(percent_mismatched) + '\n')
   #     summary.write('Reads with mismatched headers:\t' + str(nonmatching_headers_count) + '\n')
   #     summary.write('Percent with mismatched headers:\t' + str(percent_nonmatchingheaders) + '\n')
   #     
   #     # Print the summary stats for each barcode
   # 
   #     summary.write('\n\n')
   #     summary.write('Barcode\tCount\tPercent of matching barcodes\n')
   #     for item in bc_readcounts:
   #         count = bc_readcounts[item]
   #         percent_item = ( float(count) / float(matching_bcs) ) * 100
   #         itemline = item + '\t' + str(count) + '\t' + str(percent_item) + '\n'
   #         summary.write(itemline)

def parse_args():
    p = argparse.ArgumentParser(prog='mpiexec htcf-demux')
    p.add_argument('-o', '--outdir', default='out', help='default "out"')
    p.add_argument('-b', '--bcfile', default='barcodes.txt', help='default "barcodes.txt"')
    p.add_argument('-d', '--debug', action="store_true")
    p.add_argument('seqfiles', nargs='+', help='fastq file(s)')
    return p.parse_args()

if __name__ == '__main__':
    
    args = parse_args()
    w = Worker(comm, debug=args.debug)
    w.log("Started worker")
    if comm.rank == 0:
        handler = Handler(seqfiles=args.seqfiles, bcfile=args.bcfile, outdir=args.outdir)
        handler.start()

    comm.Barrier()
    w.log("Starting work")
    w.start()
    w.log("Waiting for everyone to be done.")
    comm.Barrier()
    w.log("Done with work")

    if comm.rank == 0:
        comm.send(None, 0, tag=TAG_DONE)
        for t in threading.enumerate():
            if t != threading.current_thread():
                t.join()
